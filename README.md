# Ecommerce Medallion Data Pipeline

### ğŸš€ Overview

This repository contains an end-to-end **Data Engineering pipeline** built using **Apache Airflow**, **Databricks**, **PySpark**, and **Delta Lake** based on the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).  
It demonstrates production-grade practices like incremental loads, SCD Type 2 support, SLA monitoring, failure alerts, and data quality checks.

---

## ğŸ“¦ Folder Structure

ecommerce-medallion-pipeline/
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ ecommerce_medallion_dag.py # Airflow DAG
â”‚ â”œâ”€â”€ docker-compose.yaml # Airflow setup
â”‚ â””â”€â”€ README.md # Airflow folder info
â”‚
â”œâ”€â”€ databricks/
â”‚ â”œâ”€â”€ bronze_ingestion.py # Bronze layer logic
â”‚ â”œâ”€â”€ silver_transform.py # Silver layer logic
â”‚ â””â”€â”€ gold_aggregation.py # Gold layer logic
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_data.csv # Optional sample dataset
â”‚
â”œâ”€â”€ diagrams/
â”‚ â””â”€â”€ architecture.png # Architecture diagram
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md # Project overview



## ğŸ§° Tech Stack

- **Apache Spark (PySpark)**
- **Databricks**
- **Delta Lake**
- **Apache Airflow (Docker)**
- **Docker Compose**
- **Python**
- **SQL**

---

## ğŸš¦ Features

âœ” Bronze â†’ Silver â†’ Gold layered pipeline  
âœ” Incremental ingest with **Delta MERGE**  
âœ” **SCD Type 2** implementation (Customer dimension)  
âœ” Workflow orchestration with **Airflow DAG**  
âœ” Task dependencies and SLA  
âœ” Failure alerts via email  
âœ” Data quality validations in notebooks

graph TD
    %% Define Nodes
    A[/CSV Files/]
    B[(Bronze Layer: Raw Data)]
    C[(Silver Layer: Cleaned + SCD2)]
    D[(Gold Layer: Analytics Ready)]
    E[Airflow Dashboard]

    %% Define Flow
    A -->|Ingest| B
    B -->|Transform & Historize| C
    C -->|Aggregate & Model| D
    D -->|Monitor & Visualize| E

    %% Styling
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#cd7f32,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#c0c0c0,stroke:#333,stroke-width:2px
    style D fill:#ffd700,stroke:#333,stroke-width:2px
    style E fill:#6495ed,stroke:#333,stroke-width:2px,color:#fff
---

## ğŸ“Œ Getting Started â€” Airflow


### 1. Start Airflow
```bash
cd airflow
docker compose up airflow-init
docker compose up
```
Open UI:
    a. http://localhost:8080
    b. airflow / airflow

### 2. Configure Databricks Connection

In Airflow UI:
Admin â†’ Connections â†’ Databricks
Add connection with your workspace URL and token.

### 3. Trigger Pipeline
Turn on the DAG â†’ Trigger manually â†’ Observe sequential run.

ğŸ§  How to Use Databricks Notebooks
Each Python file under /databricks represents stage logic:

- *bronze_ingestion.py*: Ingest raw data & add metadata

- *silver_transform.py*: Clean, deduplicate, apply SCD2

- *gold_aggregation.py*: Create analytics-ready tables

You can import these as notebooks or jobs in Databricks.

### ğŸ“ˆ Future Enhancements
âœ” Slack alerts / webhook notifications

âœ” Runtime metrics logging

âœ” Parameterized pipelines (e.g., date partitions)

âœ” Data quality framework (dbt, Great Expectations)

ğŸ‘¨â€ğŸ’» Author
Yukeshwaran 
Data Engineer | Spark | Databricks | Airflow | SQL

LinkedIn: https://www.linkedin.com/in/MasterYukesh/
GitHub: https://github.com/MasterYukesh
