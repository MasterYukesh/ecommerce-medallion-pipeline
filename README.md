# Ecommerce Medallion Data Pipeline

### ğŸš€ Overview

This repository contains an end-to-end **Data Engineering pipeline** built using **Apache Airflow**, **Databricks**, **PySpark**, and **Delta Lake** based on the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold).  
It demonstrates production-grade practices like incremental loads, SCD Type 2 support, SLA monitoring, failure alerts, and data quality checks.

---

## ğŸ“¦ Folder Structure

ecommerce-medallion-pipeline/
â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ ecommerce_medallion_dag.py # Airflow DAG
â”‚ â”œâ”€â”€ docker-compose.yaml # Airflow setup
â”‚ â””â”€â”€ README.md # Airflow folder info
â”‚
â”œâ”€â”€ databricks/
â”‚ â”œâ”€â”€ bronze_ingestion.py # Bronze layer logic
â”‚ â”œâ”€â”€ silver_transform.py # Silver layer logic
â”‚ â””â”€â”€ gold_aggregation.py # Gold layer logic
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_data.csv # Optional sample dataset
â”‚
â”œâ”€â”€ diagrams/
â”‚ â””â”€â”€ architecture.png # Architecture diagram
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md # Project overview



## ğŸ§° Tech Stack

- **Apache Spark (PySpark)**
- **Databricks**
- **Delta Lake**
- **Apache Airflow (Docker)**
- **Docker Compose**
- **Python**
- **SQL**

---

## ğŸš¦ Features

âœ” Bronze â†’ Silver â†’ Gold layered pipeline  
âœ” Incremental ingest with **Delta MERGE**  
âœ” **SCD Type 2** implementation (Customer dimension)  
âœ” Workflow orchestration with **Airflow DAG**  
âœ” Task dependencies and SLA  
âœ” Failure alerts via email  
âœ” Data quality validations in notebooks

---

## ğŸ“Œ Getting Started â€” Airflow

### 1. Start Airflow
```bash
cd airflow
docker compose up airflow-init
docker compose up
Open UI:

arduino
Copy code
http://localhost:8080
Login:

nginx
Copy code
airflow / airflow
2. Configure Databricks Connection
In Airflow UI:

nginx
Copy code
Admin â†’ Connections â†’ Databricks
Add connection with your workspace URL and token.

3. Trigger Pipeline
Turn on the DAG â†’ Trigger manually â†’ Observe sequential run.

ğŸ§  How to Use Databricks Notebooks
Each Python file under /databricks represents stage logic:

bronze_ingestion.py: Ingest raw data & add metadata

silver_transform.py: Clean, deduplicate, apply SCD2

gold_aggregation.py: Create analytics-ready tables

You can import these as notebooks or jobs in Databricks.

ğŸ“ˆ Future Enhancements
Slack alerts / webhook notifications

Runtime metrics logging

Parameterized pipelines (e.g., date partitions)

Data quality framework (dbt, Great Expectations)

ğŸ‘¨â€ğŸ’» Author
Yukeswaran
Data Engineer | Spark | Databricks | Airflow | SQL
LinkedIn: https://www.linkedin.com/in/MasterYukesh/
GitHub: https://github.com/MasterYukesh
